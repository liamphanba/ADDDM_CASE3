---
title: "Case Study 3"
author: "Michael Daniel Bigler and Liam Arthur Phan"
date: "`r Sys.Date()`"
output:
  rmdformats::downcute:
    code_folding: hide
    number_sections: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	include = TRUE,
	fig.align = "center")
rm(list = ls())
cat("\014")
```

# **Packages** {.unnumbered}

```{r}
library(psych)
library(corrplot)
library(ggplot2)
library(car)
library(naniar)
library(REdaS)
library(zoo)
library(foreign) 
library(lavaan)
library(ggcorrplot)
library(lares)
```

# **Data**

```{r}
df <- read.csv2('Case Study III_Structural Equation Modeling.csv', na.strings = '999', sep = ',')
df <- df[, c(1:23, 25:36)]

DT::datatable(df)
```

## Dimensions

```{r}
dim_before_na <- dim(df)
dim_before_na
```

## Summary Statistics

```{r}
DT::datatable(describe(df))
```

## Missing Analysis

```{r}
gg_miss_var(df, show_pct = TRUE)
```

This is not too bad, we can see that SAT_3 is the one with the most NA values, up to 7%. 

```{r}
naniar::vis_miss(df)
```

If we look at this plot though we see that the missing values are in a lot of the observations. Therefore, we will to handle the Confirmatory Analysis with a method for replacing those missing values. 

## Dimensions after listwise deletion

```{r}

dim_after_na <- dim(na.omit(df))
dim_after_na

na_remove_count <- dim_after_na - dim_before_na
na_remove_count[1] <- abs(na_remove_count[1])

```

Thus, we remove a lot of observations with listwise deletion, up to `r na_remove_count[1]`


```{r}
# We should not do this
#df <- na.aggregate(df)

# But this
df_listwise <- na.omit(df)
```

# **Exploratory Factor Analysis**

For exploratory factor analysis: please only consider variables image1 to image22, and use listwise deletion to handle missing data before starting exploratory factor analysis.

From Assistant

## Check Assumptions

```{r}
df_1 <- df_listwise[,1:22]
```

### Multicolinearity

```{r}

# Correlation Values Matrix
M <- cor(df_1)

# P-Value
p.mat <- cor_pmat(df_1)

```

<center>
**Correlation Plot**
</center>

```{r}

# Correlation Plot
ggcorrplot(M, hc.order = TRUE, type = "lower", lab = TRUE, p.mat = p.mat, sig.level=0.05, lab_size = 2, tl.cex = 10,outline.col = "white", ggtheme = ggplot2::theme_minimal(), colors = c("#823038", "white", "#2596be")) 

```

<center>
**Correlation Ranking**
</center>

```{r}

# Ranked Cross-Correlations
corr_cross(df_1, # name of dataset
  max_pvalue = 0.05, # display only significant correlations (at 5% level)
  top = 9 # display top 10 couples of variables (by correlation coefficient)
)

```

As we can see, We have some multicolinearity amongst the variables, at least 6 variables can be considered with high-colinearity. Im3+Im4, Im1+Im2m, Im6+Im7, Im4+Im5, Im8+Im10 and Im8+Im14. 

<center>

![](Table1.png){width=400px}

[A guide to appropriate use of Correlation coefficient in medical research](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3576830/#:~:text=A%20correlation%20coefficient%20of%20zero,between%20%E2%88%921%20and%20%2B1.)

</center>

### **Kaiser–Meyer–Olkin test (KMO)**

![](Table1.2.png){width=250px}


[KMO: Find the Kaiser, Meyer, Olkin Measure of Sampling Adequacy](https://www.rdocumentation.org/packages/psych/versions/2.3.3/topics/KMO)

**KMO Index**

```{r}
KMOTEST <- KMO(M)
sort(KMOTEST$MSAi)
```

No problem seen in each factor anti-image correlation.

**KMO Overall Measure of sampling adequacy**

```{r}
KMOTEST$MSA
```

With 0.87 sampling adequacy is very high. 

### **Bartlett’s Test of Sphericity** 

```{r}
cortest.bartlett(df_1)
```

Factor analysis can be done as the test indicates a p-value under 0 (P-value < 0).

## Factor analysis

```{r}
parallel <- fa.parallel(df_1)
parallel$fa.values
parallel$pc.values
```

FA says 2, PCA says 5

### Factors Quantity

#### Scree-plot

```{r}

fa_result <- fa(df_1, rotate = "varimax", fm = "pa")
n_factors <- length(fa_result$e.values)
scree <- data.frame(Factor_n =  as.factor(1:n_factors), Eigenvalue = fa_result$e.values)

ggplot(scree, aes(x = Factor_n, y = Eigenvalue, group = 1)) +
  geom_point() + geom_line() +
  xlab("Number of factors") +
  ylab("Initial eigenvalue") +
  labs( title = "Scree Plot",
        subtitle = "(Based on the unreduced correlation matrix)") +
  geom_hline(yintercept = 1, color="#2596be") + theme_minimal() 

```

Look at the elbow: would only select 3-4 factors

#### Kaisers-Criterion

```{r}

factors_kaiser <- sum(fa_result$e.values>1)

print(paste("Kaiser-Criterion:", factors_kaiser,"Factors"))

```

According to factor analysis six factors.


### Factors Loadings

```{r}

fa_result <- fa(df_1, rotate = "varimax", fm = "pa", nfactors = 6)
colnames(fa_result$loadings) <- c('French cuisine', 'Appealing store', 'Large assortment', 'Luxury and high quality', 'Relaxing store', 'Hip and Trendy')

cutoff_1 <- 0.3

print(fa_result$loadings, cutoff=cutoff_1,sort=TRUE)

```

With six factors we clear loadings except for some variables.

<center>

![](Table2.png){width=250px}

[Exploratory factor analysis and Cronbach’s alpha
Questionnaire Validation Workshop, 10/10/2017, USM Health Campus](https://wnarifin.github.io/workshop/qvw2017/efa.pdf)

</center>

### Communalities

```{r}

sort(fa_result$communality)

```

Only some communalities under 0.5. Need to look out for lm11, lm16, lm9 and lm16

### Seperating Factors  

```{r}

# FACTOR 1
French_cuisine <- fa_result$loadings[,1]
French_cuisine <- French_cuisine[French_cuisine>0.3]

# FACTOR 2
Appealing_store <- fa_result$loadings[,2]
Appealing_store <- Appealing_store[Appealing_store>0.3]

# FACTOR 3
Large_assortment <- fa_result$loadings[,3]
Large_assortment <- Large_assortment[Large_assortment>0.3]

# FACTOR 4
Luxury_quality <- fa_result$loadings[,4]
Luxury_quality <- Luxury_quality[Luxury_quality>0.3]

# FACTOR 5
Relaxing_store <- fa_result$loadings[,5]
Relaxing_store <- Relaxing_store[Relaxing_store>0.3]

# FACTOR 6
Hip_and_Trendy <- fa_result$loadings[,6]
Hip_and_Trendy <- Hip_and_Trendy[Hip_and_Trendy>0.3]


```

### Cronbach’s alpha

### Diagram

```{r}

fa.diagram(fa_result)

```

## Dimensions by which GLF is perceived? 

List factors that you get from the analysis

# **Confirmatory Factor Analysis**

For confirmatory factor analysis (CFA) and structural equation modeling (SEM), please use the raw data (which includes the missing values) to perform CFA and SEM, and use maximum likelihood (ML) to handle the missing data.

From Assistant

```{r}
model <- "
F1 =~Im6+Im7+Im8+Im10+Im14+Im9
F2 =~Im3+Im4+Im5
F3 =~Im1+Im2+Im15+Im16+Im19
F4 =~Im11+Im12+Im13
F5 =~Im20+Im21+Im22
F6 =~Im17+Im18+Im9+Im6
"
fit <- cfa(model, data=df_1)
summary(fit, fit.measures=TRUE, standardized=TRUE)
```

Remove Im16, Im9 and Im6

```{r}
library(dplyr)
modificationindices(fit) %>% filter(mi>10)
```

CFA looks alright we can do structure modelling

# **Structure Equation Modelling**

```{r}
model <- "
F1 =~Im6+Im7+Im8+Im10+Im14+Im9
F2 =~Im3+Im4+Im5
F3 =~Im1+Im2+Im15+Im16+Im19
F4 =~Im11+Im12+Im13
F5 =~Im20+Im21+Im22
F6 =~Im17+Im18+Im9+Im6

CS ~ F1 + F2 + F3 + F4 + F5 + F6
AC ~ F1 + F2 + F3 + F4 + F5 + F6

CS =~ SAT_1 + SAT_2 + SAT_3
AC =~ COM_A1 + COM_A2 + COM_A3 + COM_A4

RI =~ C_REP1 + C_REP2 + C_REP3
CI =~ C_CR1 + C_CR3 + C_CR4

RI ~ CS + AC + F1 + F2 + F3 + F4 + F5 + F6
CI ~ CS + AC + F1 + F2 + F3 + F4 + F5 + F6
"

fit10<-cfa(model, data=df, missing="ML")

inspect(fit10)
```

