---
title: "Case Study 3"
author: "Michael Daniel Bigler and Liam Arthur Phan"
date: "`r Sys.Date()`"
output:
  rmdformats::downcute:
    code_folding: hide
    number_sections: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	include = TRUE,
	fig.align = "center")
rm(list = ls())
cat("\014")
```

# **Packages** {.unnumbered}

```{r}
library(psych)
library(corrplot)
library(ggplot2)
library(car)
library(naniar)
library(REdaS)
library(zoo)
library(foreign) 
library(lavaan)
library(ggcorrplot)
library(lares)
library(MVN)
library(dplyr)
```

# **Data**

```{r}
df <- read.csv2('Case Study III_Structural Equation Modeling.csv', na.strings = '999', sep = ',')
df <- df[, c(1:23, 25:36)]

DT::datatable(df)
```

## Dimensions

```{r}
dim_before_na <- dim(df)
dim_before_na
```

## Summary Statistics

```{r}
DT::datatable(describe(df))
```

## Missing Analysis

```{r}
gg_miss_var(df, show_pct = TRUE)
```

This is not too bad, we can see that SAT_3 is the one with the most NA values, up to 7%.

```{r}
naniar::vis_miss(df)
```

If we look at this plot though we see that the missing values are in a lot of the observations. Therefore, we will to handle the Confirmatory Analysis with a method for replacing those missing values.

## Dimensions after listwise deletion

```{r}

dim_after_na <- dim(na.omit(df))
dim_after_na

na_remove_count <- dim_after_na - dim_before_na
na_remove_count[1] <- abs(na_remove_count[1])

```

Thus, we remove a lot of observations with listwise deletion, up to `r na_remove_count[1]`

```{r}
# We do list-wise deletion as ask by the TA
df_listwise <- na.omit(df)
```

# **Assumptions for EFA**

**From Assistant**
Please only consider variables image1 to image22, and use listwise deletion to handle missing data before starting exploratory factor analysis.


## Basic Assumptions

```{r}
df_1 <- df_listwise[,1:22]
```


### Normality - Shapiro Wilk's test

```{r}

apply(df_1, 2, shapiro.test)

```

We reject null-hypothesis for all variables and thus don't accept normality of the data. 

### Multivariate normality - Mardia's Multivariate Normality Test

To say the data are multivariate normal:

• z-kurtosis < 5 (Bentler, 2006) and the P-value should be ≥ 0.05.
• The plot should also form a straight line (Arifin, 2015).

```{r}

MVN::mvn(df_1, mvnTest = "mardia", multivariatePlot = "qq", desc = FALSE)

```

The data are not normally distributed at multivariate level. Our extraction method PAF can deal with this
non-normality. 

### Multicolinearity

```{r}

# Correlation Values Matrix
M <- cor(df_1)

# P-Value
p.mat <- cor_pmat(df_1)

```

<center>**Correlation Plot**</center>

```{r}

# Correlation Plot
ggcorrplot(M, hc.order = TRUE, type = "lower", lab = TRUE, p.mat = p.mat, sig.level=0.05, lab_size = 2, tl.cex = 10,outline.col = "white", ggtheme = ggplot2::theme_minimal(), colors = c("#823038", "white", "#2596be")) 

```

<center>**Correlation Ranking**</center>

```{r}

# Ranked Cross-Correlations
corr_cross(df_1, # name of dataset
  max_pvalue = 0.05, # display only significant correlations (at 5% level)
  top = 9 # display top 10 couples of variables (by correlation coefficient)
)


```

As we can see, We have some multicolinearity amongst the variables, at least 6 variables can be considered with high-colinearity. Im3+Im4, Im1+Im2m, Im6+Im7, Im4+Im5, Im8+Im10 and Im8+Im14.

<center>

![](Table1.png){width="400px"}

[A guide to appropriate use of Correlation coefficient in medical research](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3576830/#:~:text=A%20correlation%20coefficient%20of%20zero,between%20%E2%88%921%20and%20%2B1.)

</center>

## Factors Analysis Assumptions

### **Kaiser-Meyer-Olkin test (KMO)**

<center>

![](Table1.2.png){width="250px"}

[KMO: Find the Kaiser, Meyer, Olkin Measure of Sampling Adequacy](https://www.rdocumentation.org/packages/psych/versions/2.3.3/topics/KMO)

</center>

**KMO Index**

```{r}
KMOTEST <- KMO(M)
sort(KMOTEST$MSAi)
```

Most KMO Index are Middling, Meritorious or even Marvelous. Im6 is the lowest KMO index being Middling.

**KMO Overall Measure of sampling adequacy**

```{r}
KMOTEST$MSA
```

With 0.87, sampling adequacy is very high.

### **Bartlett's Test of Sphericity**

```{r}
cortest.bartlett(df_1)
```

EFA can be done as the test indicates a p-value under 0 (P-value \< 0) and thus can reject the null hypothesis (Identity Matrix).

# Exploratory Factor analysis

## Determine the number of factors

1. Kaiser’s eigenvalue > 1 rule.
2. Cattell’s scree test.
3. Parallel analysis.
4. Very simple structure (VSS).
5. Velicer’s minimum average partial (MAP).

### Kaiser's eigevalue > 1 rule

Factors with eigenvalues > 1 are retained. Eigenvalue can be interpreted as the proportion of the information
in a factor. The cut-off of 1 means the factor contains information = 1 item. Thus it is not worthwhile
keeping factor with information < 1 item.

```{r}

factors_kaiser <- sum(fa_result$e.values>1)

print(paste("Kaiser-Criterion:", factors_kaiser,"Factors"))

```

According to the Kaiser-Criterion, we would use 6 factors.

### Catell's scree test

We can do a factor analysis using rotation **varimax**

```{r}

fa_result <- fa(df_1, rotate = "varimax", fm = "pa")
n_factors <- length(fa_result$e.values)
scree <- data.frame(Factor_n =  as.factor(1:n_factors), Eigenvalue = fa_result$e.values)

ggplot(scree, aes(x = Factor_n, y = Eigenvalue, group = 1)) +
  geom_point() + geom_line() +
  xlab("Number of factors") +
  ylab("Initial eigenvalue") +
  labs( title = "Scree Plot",
        subtitle = "(Based on the unreduced correlation matrix)") +
  geom_hline(yintercept = 1, color="#2596be") + theme_minimal() 

```

We would say 6 factors (above blue line of eigenvalue > 1)

### Parallel analysis

```{r}

parallel <- fa.parallel(df_1, fm = "pa", fa = "fa")

print(parallel)

```

As we can see in parallel analysis, it also suggest 6 factors, nevertheless, factors up to 7 or 8 can also be considered.


### Very simple structure (VSS) criterion and Velicer’s minimum average partial (MAP) criterion

```{r}

vss(df_1, rotate = "varimax", fm = "pa")

```


VSS indicates 1/2 factors (vss1 largest at 1 and 2 factors), while MAP indicates 8 factors (map smallest at 8
factors).

[VSS criterion for the number of factors (in R's psych package)](https://stats.stackexchange.com/questions/32669/vss-criterion-for-the-number-of-factors-in-rs-psych-package)


## Extraction Method

Our data are not normally distributed, hence the extraction method of choice is principal axis factoring
(**PAF**), because it does not assume normality of data (Brown, 2015). The rotation method is **varimax**.

We run EFA by

1. fixing the number of factors as decided from previous step. 6 or 8 factors are reasonable.
2. choosing an appropriate extraction method. We use PAF, fm = "pa" (Principal Axis Factoring).
3. choosing an appropriate rotation method. We use varimax, rotate = "varimax".


### 6 Factors

We will compute the loadings with 6 factors and **varimax rotation**

**What we need to look for:**

1.  Factor loadings

Multiple threshold exist (as many rules of thumb), in our analysis we will use the standard 0.4 cut-off.

[What thresholds should I use for factor loading cut-offs?](https://imaging.mrc-cbu.cam.ac.uk/statswiki/FAQ/thresholds)

2.  Communalities

We use the standard cut-off of 0.5, all above are good.

```{r}

fa_result <- fa(df_1, nfactors = 6, fm = "pa", rotate = "varimax")

print(fa_result, cut = 0.4, digits = 3)

```

<center>

![](Table2.png){width="250px"}

[Exploratory factor analysis and Cronbach's alpha Questionnaire Validation Workshop, 10/10/2017, USM Health Campus](https://wnarifin.github.io/workshop/qvw2017/efa.pdf)

</center>

**1.  Factor loadings**

We can see that we have 3 cross-loadings, Im7, Im17 and Im19. 

Cross-Loadings (Measured with Complexity measure: *com* > 1):

**Im17 > Im19 > Im7 > 1**


**2.  Communalities**

On the table, it is column **h2** 

Low Communalities are : 

**Im9 < Im11 < Im16 < 0.5**


#### Removing Im17 (Lowest Communality and High Complexity)


```{r}

fa_result <- fa(df_1[!names(df_1) %in% c("Im17")], nfactors = 6, fm = "pa", rotate = "varimax")

print(fa_result, cut = 0.4, digits = 3)

```


#### Removing Im18 (Lowest loadings and High Complexity)


```{r}

fa_result <- fa(df_1[!names(df_1) %in% c("Im17","Im18")], nfactors = 6, fm = "pa", rotate = "varimax")

print(fa_result, cut = 0.4, digits = 3)

```


#### Removing Im8 (Cross-loadings (High Complexity))


```{r}

fa_result <- fa(df_1[!names(df_1) %in% c("Im17","Im18","Im8")], nfactors = 6, fm = "pa", rotate = "varimax")

print(fa_result, cut = 0.4, digits = 3)

```


#### Removing Im19 (Low Communality and High Complexity)


```{r}

fa_result <- fa(df_1[!names(df_1) %in% c("Im17","Im18","Im8","Im19")], nfactors = 6, fm = "pa", rotate = "varimax")

print(fa_result, cut = 0.4, digits = 3)

```

### 6 Factors - Conclusion

We removed **Im17, Im18, Im8 and Im9** until achieving clear loadings separation. 

```{r , fig.width = 6, fig.height=5}

fa.diagram(fa_result, sort = TRUE, adj = 1, rsize = 4, e.size = 0.07, main = "Factors Analysis with 6 factors", digits = 2, l.cex = 1)

```

Most Factors have good loadings (at least 2 above 0.7), while PA6 has only 2 variables loaded.


### 8 Factors 

We will redo the same analysis with 8 factors this time and using **varimax** rotation as well.

```{r}

fa_result <- fa(df_1, nfactors = 8, fm = "pa", rotate = "varimax")

print(fa_result, cut = 0.4, digits = 3)

```


**1.  Factor loadings**

We can see that we have 2 cross-loadings, Im8 and Im15. Therefore 1 less cross-loadings than 6 Factors Analysis.

Cross-Loadings (Measured with Complexity measure: *com* > 1):

**Im15 > Im8 > 1**


**2.  Communalities**

On the table, it is column **h2** 

Low Communalities are : 

**Im9 < Im11 < 0.5** (same low items communalities than in 6 Factor analysis )


#### Removing Im15 (Low Communality and High Complexity)

```{r}

fa_result <- fa(df_1[!names(df_1) %in% c("Im15")], nfactors = 8, fm = "pa", rotate = "varimax")

print(fa_result, cut = 0.4, digits = 3)

```

#### Removing Im8 (Low Communality and High Complexity)


```{r}

fa_result <- fa(df_1[!names(df_1) %in% c("Im15","Im8")], nfactors = 8, fm = "pa", rotate = "varimax")

print(fa_result, cut = 0.4, digits = 3)

```


### 8 Factors - Conclusion

We removed **Im15, Im8** until achieving clear loadings separation. Therefore we removed 2 variables less than 6 Factors Analysis done previously

```{r , fig.width = 6, fig.height=5}

fa.diagram(fa_result, sort = TRUE, adj = 1, rsize = 4, e.size = 0.07, main = "Factors Analysis with 8 factors", digits = 2, l.cex = 1)

```

Most Factors have nice loadings (at least 2 above 0.7), but **PA8** has 2 variables with only 0.62-0.69 loadings (but close to 0.7).  


### Deciding between 6 or 8 Factors

```{r}

fa_result6 <- fa(df_1[!names(df_1) %in% c("Im17","Im18","Im8","Im19")], nfactors = 6, fm = "pa", rotate = "varimax")
fa_result8 <- fa(df_1[!names(df_1) %in% c("Im15","Im8")], nfactors = 8, fm = "pa", rotate = "varimax")

fa_result6
fa_result8

```

We can see that for **6 Factors Analysis**, we obtain a **cumulative proportion variance** of 0.73. In total, the extracted factors explain **73% of the variance**.

For **8 Factors Analysis**, we obtain a **cumulative proportion variance** of 0.77. In total, the extracted factors explain **77% of the variance**.

BIC is lower with 8 factors than 6 factors, therefore may allow more generalization in future sample. 

We should also check the root mean square of residuals (RMSR). An acceptable value should be closer to 0. In **6 Factors Analysis** we have **0.067** and in **8 Factors Analysis** we have 0.052 (closer to 0). 

Finally, we must check the Tucker-Lewis Index (TLI). An acceptable value must be greater over 0.9. In **6 Factors Analysis** we have **0.946** and in **8 Factors Analysis** we have **0.964**. 

Therefore **8 Factors Analysis** is overall better, with better BIC, RMSR and TLI and also explain more the total variance with **77%**. 

[Choosing the Optimal Number of Factors in Exploratory Factor Analysis: A Model Selection Perspective](https://quantpsy.org/pubs/preacher_zhang_kim_mels_2013.pdf)


### Labeling 8 Factors 

```{r}


colnames(fa_result8$loadings) <- c("Shopping Experience", "Store Decoration","Luxury Brands","French Culture","Product Assortment","Gourmet Food","Trendiness","Professionalism")

Shopping_Experience <- c("Im20","Im21","Im22")
Store_Decoration <- c("Im3","Im4","Im5")
Luxury_Brands <- c("Im11","Im12","Im13")
French_Culture <- c("Im6","Im7","Im9")
Product_Assortment <- c("Im1","Im2")
Gourmet_Food <- c("Im10","Im14")
Trendiness <- c("Im17","Im18")
Professionalism <- c("Im16","Im19")

```



```{r , fig.width = 6, fig.height=5}

fa.diagram(fa_result8, sort = TRUE, adj = 1, rsize = 4, e.size = 0.061, main = "Conclusion of Factors Analysis - with 8 labeled factors", digits = 2, l.cex = 1)

```


### Internal consistency reliability

Our next step is to assess the internal consistency reliability of the factors that were identified through the EFA. To accomplish this, we will use **Cronbach's alpha**. We will evaluate the reliability of each factor individually by incorporating only the chosen items for that particular factor.

We need to look at:

**1. Cronbach's alpha**

The Cronbach’s alpha indicates the internal consistency reliability. The interpretation is detailed as follows
(DeVellis, 2012, pp. 95–96):

<center>
![](Table4.png){width="500px"}
</center>

**2. Corrected item-total correlation**

There are four item-total correlations provided in psych. We consider these two:

  r.cor = Item-total correlation, corrected for item overlap (Revelle, 2017). This is recommended by
Revelle (2017).

Ideally must be > 0.5 (Hair et al., 2010)


[EFA and Cronbach's alpha](https://wnarifin.github.io/workshop/qvw2017/efa.pdf)


**Shopping Experience**

```{r}
alpha.pa1 <- psych::alpha(df_1[Shopping_Experience])
alpha.pa1$total
```

raw_alpha is over 0.7 and average items correlation is above 0.5

**Store Decoration**

```{r}
alpha.pa1 <- psych::alpha(df_1[Store_Decoration])
alpha.pa1$total
```

raw_alpha is over 0.7 and average items correlation is above 0.5

**Luxury Brands**

```{r}
alpha.pa1 <- psych::alpha(df_1[Luxury_Brands])
alpha.pa1$total
```

raw_alpha is over 0.7 and average items correlation is above 0.5

**French Culture**

```{r}
alpha.pa1 <- psych::alpha(df_1[French_Culture])
alpha.pa1$total
```

raw_alpha is over 0.7 and average items correlation is above 0.5

**Product Assortment**

```{r}
alpha.pa1 <- psych::alpha(df_1[Product_Assortment])
alpha.pa1$total
```

raw_alpha is over 0.7 and average items correlation is above 0.5

**Gourmet Food**

```{r}
alpha.pa1 <- psych::alpha(df_1[Gourmet_Food])
alpha.pa1$total
```

raw_alpha is over 0.7 and average items correlation is above 0.5

**Trendiness**

```{r}
alpha.pa1 <- psych::alpha(df_1[Trendiness])
alpha.pa1$total
```

raw_alpha is over 0.7 and average items correlation is above 0.5

**Professionalism**

```{r}
alpha.pa1 <- psych::alpha(df_1[Professionalism])
alpha.pa1$total
```

raw_alpha is over 0.7 and average items correlation is above 0.5


Our assessment suggests that the factors extracted are reliable, and therefore it is advisable to retain all the items related to these factors.


## Dimensions by which Galeries Layfayette is perceived?

```{r , fig.width = 6, fig.height=5}

fa.diagram(fa_result8, sort = TRUE, adj = 1, rsize = 4, e.size = 0.061, main = "Galeries Lafayette - Perception Dimensions", digits = 2, l.cex = 1)

```

**Dimensions Definitions:**

  **Product Assortment**: This group pertains to the variety and range of products offered by the store.

  **Store Decoration**: This group pertains to the aesthetic elements of the store's interior and exterior, such as the artistic and creative decoration of the sales area, and the appealing arrangement of shop windows.

  **French Culture**: This group pertains to elements of French culture, such as French savoir-vivre, fashion. 

  **Gourmet Food**: This group pertains to high-quality food and cosmetic products offered by the store.

  **Luxury Brands**: This group pertains to the presence of luxury and designer brands in the store.

  **Professionalism**: This group pertains to elements of professionalism, such as the store's professional appearance towards customers and professional organization.

  **Trendiness**: This group pertains to the store's ability to stay current and up-to-date with the latest trends in the market.

  **Shopping Experience**: This group pertains to the overall shopping experience, including elements such as relaxing shopping, a great place to stroll, and an intimate shop atmosphere.


# **Confirmatory Factor Analysis**

**From Assistant**
For confirmatory factor analysis (CFA) and structural equation modeling (SEM), please use the raw data (which includes the missing values) to perform CFA and SEM, and use maximum likelihood (ML) to handle the missing data.

```{r}
model_CFA <-"
Shopping_Experience =~ Im20+Im21+Im22
Store_Decoration =~ Im3+Im4+Im5
Luxury_Brands =~ Im11+Im12+Im13
French_Culture =~ Im6+Im7+Im9
Product_Assortment =~ Im1+Im2
Gourmet_Food =~ Im10+Im14
Trendiness =~ Im17+Im18
Professionalism =~ Im16+Im19"

fit <- cfa(model_CFA, data=df, missing="ML")

inspect(fit)

summary(fit, fit.measures=TRUE, standardized=TRUE)

```



```{r}
modificationindices(fit) %>% filter(mi>10)
```

CFA looks alright we can do structure modelling

# **Structure Equation Modelling**

```{r}
model <- "
F1 =~Im6+Im7+Im8+Im10+Im14+Im9
F2 =~Im3+Im4+Im5
F3 =~Im1+Im2+Im15+Im16+Im19
F4 =~Im11+Im12+Im13
F5 =~Im20+Im21+Im22
F6 =~Im17+Im18+Im9+Im6

CS ~ F1 + F2 + F3 + F4 + F5 + F6
AC ~ F1 + F2 + F3 + F4 + F5 + F6

CS =~ SAT_1 + SAT_2 + SAT_3
AC =~ COM_A1 + COM_A2 + COM_A3 + COM_A4

RI =~ C_REP1 + C_REP2 + C_REP3
CI =~ C_CR1 + C_CR3 + C_CR4

RI ~ CS + AC + F1 + F2 + F3 + F4 + F5 + F6
CI ~ CS + AC + F1 + F2 + F3 + F4 + F5 + F6
"

fit10<-cfa(model, data=df, missing="ML")

inspect(fit10)
```
