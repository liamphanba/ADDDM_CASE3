---
title: "Case Study 3"
author: "Michael Daniel Bigler and Liam Arthur Phan"
date: "`r Sys.Date()`"
output:
  rmdformats::downcute:
    code_folding: hide
    number_sections: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	include = TRUE,
	fig.align = "center")
rm(list = ls())
cat("\014")
```

# **Packages** {.unnumbered}

```{r}
library(psych)
library(corrplot)
library(ggplot2)
library(car)
library(naniar)
library(REdaS)
library(zoo)
library(foreign) 
library(lavaan)
library(ggcorrplot)
library(lares)
library(MVN)
```

# **Data**

```{r}
df <- read.csv2('Case Study III_Structural Equation Modeling.csv', na.strings = '999', sep = ',')
df <- df[, c(1:23, 25:36)]

DT::datatable(df)
```

## Dimensions

```{r}
dim_before_na <- dim(df)
dim_before_na
```

## Summary Statistics

```{r}
DT::datatable(describe(df))
```

## Missing Analysis

```{r}
gg_miss_var(df, show_pct = TRUE)
```

This is not too bad, we can see that SAT_3 is the one with the most NA values, up to 7%.

```{r}
naniar::vis_miss(df)
```

If we look at this plot though we see that the missing values are in a lot of the observations. Therefore, we will to handle the Confirmatory Analysis with a method for replacing those missing values.

## Dimensions after listwise deletion

```{r}

dim_after_na <- dim(na.omit(df))
dim_after_na

na_remove_count <- dim_after_na - dim_before_na
na_remove_count[1] <- abs(na_remove_count[1])

```

Thus, we remove a lot of observations with listwise deletion, up to `r na_remove_count[1]`

```{r}
# We do list-wise deletion as ask by the TA
df_listwise <- na.omit(df)
```

# **Assumptions for EFA**

**From Assistant**
Please only consider variables image1 to image22, and use listwise deletion to handle missing data before starting exploratory factor analysis.


## Basic Assumptions

```{r}
df_1 <- df_listwise[,1:22]
```


### Normality - Shapiro Wilk's test

```{r}

apply(df_1, 2, shapiro.test)

```

We reject null-hypothesis for all variables and thus don't accept normality of the data. 

### Multivariate normality - Mardia's Multivariate Normality Test

To say the data are multivariate normal:

• z-kurtosis < 5 (Bentler, 2006) and the P-value should be ≥ 0.05.
• The plot should also form a straight line (Arifin, 2015).

```{r}

MVN::mvn(df_1, mvnTest = "mardia", multivariatePlot = "qq", desc = FALSE)

```

The data are not normally distributed at multivariate level. Our extraction method PAF can deal with this
non-normality. 

### Multicolinearity

```{r}

# Correlation Values Matrix
M <- cor(df_1)

# P-Value
p.mat <- cor_pmat(df_1)

```

<center>**Correlation Plot**</center>

```{r}

# Correlation Plot
ggcorrplot(M, hc.order = TRUE, type = "lower", lab = TRUE, p.mat = p.mat, sig.level=0.05, lab_size = 2, tl.cex = 10,outline.col = "white", ggtheme = ggplot2::theme_minimal(), colors = c("#823038", "white", "#2596be")) 

```

<center>**Correlation Ranking**</center>

```{r}

# Ranked Cross-Correlations
corr_cross(df_1, # name of dataset
  max_pvalue = 0.05, # display only significant correlations (at 5% level)
  top = 9 # display top 10 couples of variables (by correlation coefficient)
)


```

As we can see, We have some multicolinearity amongst the variables, at least 6 variables can be considered with high-colinearity. Im3+Im4, Im1+Im2m, Im6+Im7, Im4+Im5, Im8+Im10 and Im8+Im14.

<center>

![](Table1.png){width="400px"}

[A guide to appropriate use of Correlation coefficient in medical research](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3576830/#:~:text=A%20correlation%20coefficient%20of%20zero,between%20%E2%88%921%20and%20%2B1.)

</center>

## Factors Analysis Assumptions

### **Kaiser-Meyer-Olkin test (KMO)**

<center>

![](Table1.2.png){width="250px"}

[KMO: Find the Kaiser, Meyer, Olkin Measure of Sampling Adequacy](https://www.rdocumentation.org/packages/psych/versions/2.3.3/topics/KMO)

</center>

**KMO Index**

```{r}
KMOTEST <- KMO(M)
sort(KMOTEST$MSAi)
```

Most KMO Index are Middling, Meritorious or even Marvelous. Im6 is the lowest KMO index being Middling.

**KMO Overall Measure of sampling adequacy**

```{r}
KMOTEST$MSA
```

With 0.87, sampling adequacy is very high.

### **Bartlett's Test of Sphericity**

```{r}
cortest.bartlett(df_1)
```

EFA can be done as the test indicates a p-value under 0 (P-value \< 0) and thus can reject the null hypothesis (Identity Matrix).

## Exploratory Factor analysis

### Determine the number of factors

1. Kaiser’s eigenvalue > 1 rule.
2. Cattell’s scree test.
3. Parallel analysis.
4. Very simple structure (VSS).
5. Velicer’s minimum average partial (MAP).

#### Kaiser's eigevalue > 1 rule

Factors with eigenvalues > 1 are retained. Eigenvalue can be interpreted as the proportion of the information
in a factor. The cut-off of 1 means the factor contains information = 1 item. Thus it is not worthwhile
keeping factor with information < 1 item.

```{r}

factors_kaiser <- sum(fa_result$e.values>1)

print(paste("Kaiser-Criterion:", factors_kaiser,"Factors"))

```

According to the Kaiser-Criterion, we would use 6 factors.

#### Catell's scree test

We can do a factor analysis using rotation **varimax**

```{r}

fa_result <- fa(df_1, rotate = "varimax", fm = "pa")
n_factors <- length(fa_result$e.values)
scree <- data.frame(Factor_n =  as.factor(1:n_factors), Eigenvalue = fa_result$e.values)

ggplot(scree, aes(x = Factor_n, y = Eigenvalue, group = 1)) +
  geom_point() + geom_line() +
  xlab("Number of factors") +
  ylab("Initial eigenvalue") +
  labs( title = "Scree Plot",
        subtitle = "(Based on the unreduced correlation matrix)") +
  geom_hline(yintercept = 1, color="#2596be") + theme_minimal() 

```

We would say 6 factors (above blue line of eigenvalue > 1)

#### Parallel analysis

```{r}

parallel <- fa.parallel(df_1, fm = "pa", fa = "fa")

print(parallel)

```

As we can see in parallel analysis, it also suggest 6 factors, nevertheless, factors up to 7 or 8 can also be considered.


#### Very simple structure (VSS) criterion and Velicer’s minimum average partial (MAP) criterion

```{r}

vss(df_1, rotate = "varimax", fm = "pa")

```


VSS indicates 1/2 factors (vss1 largest at 1 and 2 factors), while MAP indicates 8 factors (map smallest at 8
factors).

[VSS criterion for the number of factors (in R's psych package)](https://stats.stackexchange.com/questions/32669/vss-criterion-for-the-number-of-factors-in-rs-psych-package)


### Extraction Method

Our data are not normally distributed, hence the extraction method of choice is principal axis factoring
(**PAF**), because it does not assume normality of data (Brown, 2015). The rotation method is **varimax**.

We run EFA by

1. fixing the number of factors as decided from previous step. 6 or 8 factors are reasonable.
2. choosing an appropriate extraction method. We use PAF, fm = "pa" (Principal Axis Factoring).
3. choosing an appropriate rotation method. We use varimax, rotate = "varimax".


#### 6 Factors

We will compute the loadings with 6 factors and **varimax rotation**

**What we need to look for:**

1.  Factor loadings

Multiple threshold exist (as many rules of thumb), in our analysis we will use the standard 0.4 cut-off.

[What thresholds should I use for factor loading cut-offs?](https://imaging.mrc-cbu.cam.ac.uk/statswiki/FAQ/thresholds)

2.  Communalities

We use the standard cut-off of 0.5, all above are good.

```{r}

fa_result <- fa(df_1, nfactors = 6, fm = "pa", rotate = "varimax")

print(fa_result, cut = 0.4, digits = 3)

```

<center>

![](Table2.png){width="250px"}

[Exploratory factor analysis and Cronbach's alpha Questionnaire Validation Workshop, 10/10/2017, USM Health Campus](https://wnarifin.github.io/workshop/qvw2017/efa.pdf)

</center>

**1.  Factor loadings**

We can see that we have 3 cross-loadings, Im7, Im17 and Im19. 

Cross-Loadings (Measured with Complexity measure: *com* > 1):

**Im17 > Im19 > Im7 > 1**


**2.  Communalities**

On the table, it is column **h2** 

Low Communalities are : 

**Im9 < Im11 < Im16 < 0.5**


##### Removing Im17 (Lowest Communality and High Complexity)


```{r}

fa_result <- fa(df_1[!names(df_1) %in% c("Im17")], nfactors = 6, fm = "pa", rotate = "varimax")

print(fa_result, cut = 0.4, digits = 3)

```


##### Removing Im18 (Lowest loadings and High Complexity)


```{r}

fa_result <- fa(df_1[!names(df_1) %in% c("Im17","Im18")], nfactors = 6, fm = "pa", rotate = "varimax")

print(fa_result, cut = 0.4, digits = 3)

```


##### Removing Im8 (Cross-loadings (High Complexity))


```{r}

fa_result <- fa(df_1[!names(df_1) %in% c("Im17","Im18","Im8")], nfactors = 6, fm = "pa", rotate = "varimax")

print(fa_result, cut = 0.4, digits = 3)

```


##### Removing Im19 (Low Communality and High Complexity)


```{r}

fa_result <- fa(df_1[!names(df_1) %in% c("Im17","Im18","Im8","Im19")], nfactors = 6, fm = "pa", rotate = "varimax")

print(fa_result, cut = 0.4, digits = 3)

```

#### 6 Factors - Conclusion

We removed **Im17, Im18, Im8 and Im9** until achieving clear loadings separation. 

```{r , fig.width = 6, fig.height=5}

fa.diagram(fa_result, sort = TRUE, adj = 1, rsize = 4, e.size = 0.07, main = "Factors Analysis with 6 factors", digits = 2, l.cex = 1)

```

Most Factors have nice loadings (at least 2 above 0.7)l, while PA6 has only 2 variables loaded.


#### 8 Factors 

We will redo the same analysis with 8 factors this time and using **varimax** rotation as well.

```{r}

fa_result <- fa(df_1, nfactors = 8, fm = "pa", rotate = "varimax")

print(fa_result, cut = 0.4, digits = 3)

```


**1.  Factor loadings**

We can see that we have 3 cross-loadings, Im7, Im17 and Im19. 

Cross-Loadings (Measured with Complexity measure: *com* > 1):

**Im17 > Im19 > Im7 > 1**


**2.  Communalities**

On the table, it is column **h2** 

Low Communalities are : 

**Im9 < Im11 < Im16 < 0.5**







### Factors Names

```{r}

colnames(fa_result$loadings) <- c('French cuisine', 'Appealing store', 'Large assortment', 'Luxury and high quality', 'Relaxing store', 'Hip and Trendy')


# FACTOR 1
French_cuisine <- fa_result$loadings[,1]
French_cuisine <- French_cuisine[French_cuisine>0.3]

French_cuisine_Names <- names(French_cuisine)

# FACTOR 2
Appealing_store <- fa_result$loadings[,2]
Appealing_store <- Appealing_store[Appealing_store>0.3]

Appealing_store_Names <- names(Appealing_store)

# FACTOR 3
Large_assortment <- fa_result$loadings[,3]
Large_assortment <- Large_assortment[Large_assortment>0.3]

Large_assortment_Names <- names(Large_assortment)

# FACTOR 4
Luxury_quality <- fa_result$loadings[,4]
Luxury_quality <- Luxury_quality[Luxury_quality>0.3]

Luxury_quality_Names <- names(Luxury_quality)

# FACTOR 5
Relaxing_store <- fa_result$loadings[,5]
Relaxing_store <- Relaxing_store[Relaxing_store>0.3]

Relaxing_store_Names <- names(Relaxing_store)

# FACTOR 6
Hip_and_Trendy <- fa_result$loadings[,6]
Hip_and_Trendy <- Hip_and_Trendy[Hip_and_Trendy>0.3]

Hip_and_Trendy_Names <- names(Hip_and_Trendy)


```

### Cronbach's alpha

<center>![](Table4.png){width="500px"}</center>

**French cuisine**

```{r}
alpha.pa1 = psych::alpha(df_1[French_cuisine_Names])
alpha.pa1$item.stats
```

**Appealing store**

```{r}
alpha.pa1 = psych::alpha(df_1[Appealing_store_Names])
alpha.pa1$item.stats
```

**Large assortment**

```{r}
alpha.pa1 = psych::alpha(df_1[Large_assortment_Names])
alpha.pa1$item.stats
```

**Luxury and High quality**

```{r}
alpha.pa1 = psych::alpha(df_1[Luxury_quality_Names])
alpha.pa1$item.stats
```

**Relaxing store**

```{r}
alpha.pa1 = psych::alpha(df_1[Relaxing_store_Names])
alpha.pa1$item.stats
```

**Hip and Trendy**

```{r}
alpha.pa1 = psych::alpha(df_1[Hip_and_Trendy_Names])
alpha.pa1$item.stats
```

### Diagram

```{r}

fa.diagram(fa_result)

```

## Dimensions by which GLF is perceived?

List factors that you get from the analysis

# **Confirmatory Factor Analysis**

For confirmatory factor analysis (CFA) and structural equation modeling (SEM), please use the raw data (which includes the missing values) to perform CFA and SEM, and use maximum likelihood (ML) to handle the missing data.

From Assistant

```{r}
model <- "
F1 =~Im6+Im7+Im8+Im10+Im14+Im9
F2 =~Im3+Im4+Im5
F3 =~Im1+Im2+Im15+Im16+Im19
F4 =~Im11+Im12+Im13
F5 =~Im20+Im21+Im22
F6 =~Im17+Im18+Im9+Im6
"
fit <- cfa(model, data=df_1)
summary(fit, fit.measures=TRUE, standardized=TRUE)
```

Remove Im16, Im9 and Im6

```{r}
library(dplyr)
modificationindices(fit) %>% filter(mi>10)
```

CFA looks alright we can do structure modelling

# **Structure Equation Modelling**

```{r}
model <- "
F1 =~Im6+Im7+Im8+Im10+Im14+Im9
F2 =~Im3+Im4+Im5
F3 =~Im1+Im2+Im15+Im16+Im19
F4 =~Im11+Im12+Im13
F5 =~Im20+Im21+Im22
F6 =~Im17+Im18+Im9+Im6

CS ~ F1 + F2 + F3 + F4 + F5 + F6
AC ~ F1 + F2 + F3 + F4 + F5 + F6

CS =~ SAT_1 + SAT_2 + SAT_3
AC =~ COM_A1 + COM_A2 + COM_A3 + COM_A4

RI =~ C_REP1 + C_REP2 + C_REP3
CI =~ C_CR1 + C_CR3 + C_CR4

RI ~ CS + AC + F1 + F2 + F3 + F4 + F5 + F6
CI ~ CS + AC + F1 + F2 + F3 + F4 + F5 + F6
"

fit10<-cfa(model, data=df, missing="ML")

inspect(fit10)
```
